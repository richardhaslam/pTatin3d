** ====================================================================================== 
**
**             ___________                          _______
**     _______/          /_____ ________ __ _   ___/       \ ____
**    /      /___    ___/      /__   __/  /  \ /  /\__ _   /     \
**   /  //  /   /   /  /  //  /  /  / /  /    /  /___/_   /  //  /
**  /  ___ /   /   /  /  _   /  /  / /  /  /    //       /  //  /
** /__/       /___/  /__//__/  /__/ /__/__/ \__//_______/______/
**
** Authors:  Dave A. May          (dave.may@erdw.ethz.ch)           
**           Laetitia Le Pourhiet (laetitia.le_pourhiet@upmc.fr)    
**           Jed Brown            (jedbrown@mcs.anl.gov)            
**
** git url: https://bitbucket.org/jedbrown/ptatin3d.git 
** commit hash: [out-of-date] Execute "make releaseinfo" to update to the most recent revision 
** log: [out-of-date] Execute "make releaseinfo" to update to the most recent revision 
**                                                                       
** TATIN_CFLAGS = -std=gnu99 -O0 -g -Wall -Wno-unused-variable
**                                                                       
** WARNING pTatin3d appears to have been compiled with debug options 
** For significant performance improvements, please consult the file makefile.arch  
** Adjust TATIN_CFLAGS to include aggressive compiler optimizations 
**                                                                       
** ====================================================================================== 
RheologyConstantsInitialise: global viscosity cut-off, min= 1.000000e-100, max = 1.000000e+100  
[pTatin] Writing output to existing directory: pt3dout 
[pTatin] Created log file: pt3dout/ptatin.log-2015.07.15_15:39:35 
[pTatin] Writing output to existing directory: pt3dout 
[pTatin] Created options file: pt3dout/ptatin.options-2015.07.15_15:39:35 
[pTatin] Created options file: pt3dout/ptatin.options 
  [pTatinModel]: Registering model [0] with name "template"
  [pTatinModel]: Registering model [1] with name "viscous_sinker"
  [pTatinModel]: Registering model [2] with name "Gene3D"
  [pTatinModel]: Registering model [3] with name "Gene3DNueve"
  [pTatinModel]: Registering model [4] with name "indentor"
  [pTatinModel]: Registering model [5] with name "rift3D"
  [pTatinModel]: Registering model [6] with name "rift3D_T"
  [pTatinModel]: Registering model [7] with name "sierra"
  [pTatinModel]: Registering model [8] with name "advdiff_example"
  [pTatinModel]: Registering model [9] with name "delamination"
  [pTatinModel]: Registering model [10] with name "Riftrh"
  [pTatinModel]: Registering model [11] with name "geomod2008"
  [pTatinModel]: Registering model [12] with name "multilayer_folding"
  [pTatinModel]: Registering model [13] with name "submarinelavaflow"
  [pTatinModel]: Registering model [14] with name "ex_subduction"
  [pTatinModel]: Registering model [15] with name "iplus"
  [pTatinModel]: Registering model [16] with name "subduction_initiation2d"
  [pTatinModel]: Registering model [17] with name "convection2d"
  [pTatinModel]: Registering model [18] with name "thermal_sb"
  [pTatinModel]: Registering model [19] with name "sd3d"
  [pTatinModel]: Registering model [20] with name "pas"
  [pTatinModel]: Registering model [21] with name "pd"
  [pTatinModel]: -ptatin_model "viscous_sinker" was detected
pTatin3dRestart: Required to specify a suffix for your restart file via -restart_prefix
pTatin3dRestart: Unable to restart job
[[ModelInitialize_ViscousSinker]]
BCList: Mem. usage (min,max) = 4.56e-01,5.42e-01 (MB) 
VolumeQuadratureCreate_GaussLegendreStokes:
	Using 3x3 pnt Gauss Legendre quadrature
DataBucketView(MPI): ("GaussLegendre StokesCoefficients")
  L                  = 110592 
  buffer (max)       = 8 
  allocated          = 110600 
  nfields registered = 1 
    [  0]: field name  ==>>              QPntVolCoefStokes : Mem. usage = 6.64e-01 (MB) : rank0
  Total mem. usage                                                      = 5.31e+00 (MB) : collective
DataBucketView(MPI): ("SurfaceGaussLegendre StokesCoefficients[face 0]")
  L                  = 2304 
  buffer (max)       = 8 
  allocated          = 2316 
  nfields registered = 1 
    [  0]: field name  ==>>             QPntSurfCoefStokes : Mem. usage = 2.24e-04 (MB) : rank0
  Total mem. usage                                                      = 2.59e-01 (MB) : collective
DataBucketView(MPI): ("SurfaceGaussLegendre StokesCoefficients[face 1]")
  L                  = 2304 
  buffer (max)       = 8 
  allocated          = 2316 
  nfields registered = 1 
    [  0]: field name  ==>>             QPntSurfCoefStokes : Mem. usage = 6.46e-02 (MB) : rank0
  Total mem. usage                                                      = 2.59e-01 (MB) : collective
DataBucketView(MPI): ("SurfaceGaussLegendre StokesCoefficients[face 2]")
  L                  = 2304 
  buffer (max)       = 8 
  allocated          = 2316 
  nfields registered = 1 
    [  0]: field name  ==>>             QPntSurfCoefStokes : Mem. usage = 2.24e-04 (MB) : rank0
  Total mem. usage                                                      = 2.59e-01 (MB) : collective
DataBucketView(MPI): ("SurfaceGaussLegendre StokesCoefficients[face 3]")
  L                  = 2304 
  buffer (max)       = 8 
  allocated          = 2316 
  nfields registered = 1 
    [  0]: field name  ==>>             QPntSurfCoefStokes : Mem. usage = 6.46e-02 (MB) : rank0
  Total mem. usage                                                      = 2.59e-01 (MB) : collective
DataBucketView(MPI): ("SurfaceGaussLegendre StokesCoefficients[face 4]")
  L                  = 2304 
  buffer (max)       = 8 
  allocated          = 2316 
  nfields registered = 1 
    [  0]: field name  ==>>             QPntSurfCoefStokes : Mem. usage = 2.24e-04 (MB) : rank0
  Total mem. usage                                                      = 2.59e-01 (MB) : collective
DataBucketView(MPI): ("SurfaceGaussLegendre StokesCoefficients[face 5]")
  L                  = 2304 
  buffer (max)       = 8 
  allocated          = 2316 
  nfields registered = 1 
    [  0]: field name  ==>>             QPntSurfCoefStokes : Mem. usage = 6.46e-02 (MB) : rank0
  Total mem. usage                                                      = 2.59e-01 (MB) : collective
  MaterialPointsStokes: Using Q1 projection
[[Swarm initialization: 0.0005 (sec)]]
SwarmMPntStd_AssignUniquePointIdentifiers : max_pid = 0 
[[Swarm->coordinate assignment: 4096 points : 0.0042 (sec)]]
************************** Starting _DataExCompleteCommunicationMap ************************** 
max_nnz = 7 
Mat Object: 8 MPI processes
  type: mpiaij
  rows=8, cols=8
  total: nonzeros=56, allocated nonzeros=64
  total number of mallocs used during MatSetValues calls =0
    not using I-node (on process 0) routines
************************** Ending _DataExCompleteCommunicationMap [setup time: 1.1221e-01 (sec)] ************************** 
[[SwarmDMDA3dDataExchangerCreate: time = 1.1286e-01 (sec)]]
[[ModelApplyInitialMeshGeometry_ViscousSinker]]
RUNNING DEFORMED MESH EXAMPLE 
[[ViscousSinker_ApplyInitialMaterialGeometry_SingleInclusion]]
[[ModelApplyBoundaryCondition_ViscousSinker]]
Mesh size (16 x 16 x 16) : MG levels 3  
         level [ 0]: global Q2 elements (2 x 8 x 2) 
         level [ 1]: global Q2 elements (4 x 16 x 4) 
         level [ 2]: global Q2 elements (16 x 16 x 16) 
[r   0]: level [ 0]: local Q2 elements  (1 x 4 x 1) 
[r   0]: level [ 1]: local Q2 elements  (2 x 8 x 2) 
[r   0]: level [ 2]: local Q2 elements  (8 x 8 x 8) 
[r   0]: level [ 0]: element range [0 - 0] x [0 - 3] x [0 - 0] 
[r   0]: level [ 1]: element range [0 - 1] x [0 - 7] x [0 - 1] 
[r   0]: level [ 2]: element range [0 - 7] x [0 - 7] x [0 - 7] 
VolumeQuadratureCreate_GaussLegendreStokes:
	Using 3x3 pnt Gauss Legendre quadrature
DataBucketView(MPI): ("GaussLegendre StokesCoefficients")
  L                  = 864 
  buffer (max)       = 8 
  allocated          = 872 
  nfields registered = 1 
    [  0]: field name  ==>>              QPntVolCoefStokes : Mem. usage = 5.23e-03 (MB) : rank0
  Total mem. usage                                                      = 4.19e-02 (MB) : collective
VolumeQuadratureCreate_GaussLegendreStokes:
	Using 3x3 pnt Gauss Legendre quadrature
DataBucketView(MPI): ("GaussLegendre StokesCoefficients")
  L                  = 6912 
  buffer (max)       = 8 
  allocated          = 6920 
  nfields registered = 1 
    [  0]: field name  ==>>              QPntVolCoefStokes : Mem. usage = 4.15e-02 (MB) : rank0
  Total mem. usage                                                      = 3.32e-01 (MB) : collective
BCList: Mem. usage (min,max) = 7.68e-03,1.48e-02 (MB) 
BCList: Mem. usage (min,max) = 3.87e-02,5.90e-02 (MB) 
[[ModelApplyBoundaryConditionMG_ViscousSinker]]
[[ModelInitialCondition_ViscousSinker]]
*** Rheology update for RHEOLOGY_VISCOUS selected ***
Update rheology (viscous) [mpoint]: (min,max)_eta 1.00e-03,1.00e+00; log10(max/min) 3.00e+00; cpu time 1.11e-03 (sec)
Level [2]: Coarse grid type :: Re-discretisation :: matrix free operator 
Level [1]: Coarse grid type :: Re-discretisation :: assembled operator 
Level [0]: Coarse grid type :: Galerkin :: assembled operator 
[[ModelOutput_ViscousSinker]]
  writing pvdfilename pt3dout/timeseries_vp.pvd 
[[DESIGN FLAW]] pTatin3d_ModelOutput_VelocityPressure_Stokes: require better physics modularity to extract (u,p) <---| (X) 
[[DESIGN FLAW]] pTatinOutputMeshVelocityPressureVTS_v0_binary: only printing P0 component of pressure field 
pTatin3d_ModelOutput_VelocityPressure_Stokes() -> icbc_vp.(pvd,pvts,vts): CPU time 4.49e-02 (sec) 
  writing pvdfilename pt3dout/timeseries_mpoints_std.pvd 
pTatin3d_ModelOutput_MPntStd() -> icbc_mpoints_std.(pvd,pvtu,vtu): CPU time 1.47e-02 (sec) 
   [[ COMPUTING FLOW FIELD FOR STEP : 0 ]]
Update rheology (viscous) [mpoint]: (min,max)_eta 1.00e-03,1.00e+00; log10(max/min) 3.00e+00; cpu time 1.02e-03 (sec)
Update rheology (viscous) [mpoint]: (min,max)_eta 1.00e-03,1.00e+00; log10(max/min) 3.00e+00; cpu time 1.07e-03 (sec)
    0 KSP Residual norm 0.00136154 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1 
      1 KSP Residual norm 0.999385 
      2 KSP Residual norm 0.966711 
    1 KSP Residual norm 0.00134754 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 0.766414 
      1 KSP Residual norm 0.210816 
      2 KSP Residual norm 0.143872 
    2 KSP Residual norm 0.00134119 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.7071 
      1 KSP Residual norm 0.310318 
      2 KSP Residual norm 0.140504 
    3 KSP Residual norm 0.00132768 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 2.66941 
      1 KSP Residual norm 0.348154 
      2 KSP Residual norm 0.164905 
    4 KSP Residual norm 0.00132587 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 2.08016 
      1 KSP Residual norm 0.1583 
      2 KSP Residual norm 0.0596081 
    5 KSP Residual norm 0.00132283 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.72068 
      1 KSP Residual norm 0.250524 
      2 KSP Residual norm 0.102618 
    6 KSP Residual norm 0.00132282 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.70363 
      1 KSP Residual norm 0.316522 
      2 KSP Residual norm 0.102933 
    7 KSP Residual norm 0.00131784 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 2.31799 
      1 KSP Residual norm 0.312328 
      2 KSP Residual norm 0.0625906 
    8 KSP Residual norm 0.00128192 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 2.53305 
      1 KSP Residual norm 0.247114 
      2 KSP Residual norm 0.0763339 
    9 KSP Residual norm 0.0012512 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 2.05121 
      1 KSP Residual norm 0.241698 
      2 KSP Residual norm 0.0822688 
   10 KSP Residual norm 0.00112467 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.89844 
      1 KSP Residual norm 0.210895 
      2 KSP Residual norm 0.0549383 
   11 KSP Residual norm 0.000920171 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.59034 
      1 KSP Residual norm 0.29564 
      2 KSP Residual norm 0.109962 
   12 KSP Residual norm 0.000652597 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.86013 
      1 KSP Residual norm 0.243708 
      2 KSP Residual norm 0.102011 
   13 KSP Residual norm 0.000513751 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 2.26902 
      1 KSP Residual norm 0.307081 
      2 KSP Residual norm 0.0758268 
   14 KSP Residual norm 0.000362023 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.70654 
      1 KSP Residual norm 0.196423 
      2 KSP Residual norm 0.113659 
   15 KSP Residual norm 0.000212726 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.50582 
      1 KSP Residual norm 0.163042 
      2 KSP Residual norm 0.0936338 
   16 KSP Residual norm 0.000120533 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.69024 
      1 KSP Residual norm 0.310908 
      2 KSP Residual norm 0.158326 
   17 KSP Residual norm 8.6178e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.857 
      1 KSP Residual norm 0.342614 
      2 KSP Residual norm 0.189259 
   18 KSP Residual norm 7.20463e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.16527 
      1 KSP Residual norm 0.17385 
      2 KSP Residual norm 0.16031 
   19 KSP Residual norm 7.16807e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.37647 
      1 KSP Residual norm 0.112001 
      2 KSP Residual norm 0.0524558 
   20 KSP Residual norm 7.16774e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.18769 
      1 KSP Residual norm 0.13856 
      2 KSP Residual norm 0.0492147 
   21 KSP Residual norm 7.14143e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 0.963658 
      1 KSP Residual norm 0.106891 
      2 KSP Residual norm 0.0596951 
   22 KSP Residual norm 6.74169e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.30904 
      1 KSP Residual norm 0.109388 
      2 KSP Residual norm 0.0426383 
   23 KSP Residual norm 4.72421e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.27645 
      1 KSP Residual norm 0.177878 
      2 KSP Residual norm 0.104395 
   24 KSP Residual norm 2.49765e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.11509 
      1 KSP Residual norm 0.25036 
      2 KSP Residual norm 0.231281 
   25 KSP Residual norm 1.72573e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.23704 
      1 KSP Residual norm 0.286116 
      2 KSP Residual norm 0.256215 
   26 KSP Residual norm 1.29637e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.07016 
      1 KSP Residual norm 0.231779 
      2 KSP Residual norm 0.215559 
   27 KSP Residual norm 1.0999e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.52145 
      1 KSP Residual norm 0.179749 
      2 KSP Residual norm 0.128848 
   28 KSP Residual norm 7.69586e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.09106 
      1 KSP Residual norm 0.2248 
      2 KSP Residual norm 0.218933 
   29 KSP Residual norm 5.36008e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.38928 
      1 KSP Residual norm 0.362369 
      2 KSP Residual norm 0.311155 
   30 KSP Residual norm 3.95292e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 0.787234 
      1 KSP Residual norm 0.604827 
      2 KSP Residual norm 0.594809 
   31 KSP Residual norm 3.90734e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 0.438189 
      1 KSP Residual norm 0.100354 
      2 KSP Residual norm 0.0786926 
   32 KSP Residual norm 3.87978e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.16469 
      1 KSP Residual norm 0.200291 
      2 KSP Residual norm 0.105933 
   33 KSP Residual norm 3.85955e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.5258 
      1 KSP Residual norm 0.181746 
      2 KSP Residual norm 0.057758 
   34 KSP Residual norm 3.83246e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.19698 
      1 KSP Residual norm 0.0787679 
      2 KSP Residual norm 0.0354814 
   35 KSP Residual norm 3.66619e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.0036 
      1 KSP Residual norm 0.213118 
      2 KSP Residual norm 0.104511 
   36 KSP Residual norm 3.4931e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.31574 
      1 KSP Residual norm 0.210048 
      2 KSP Residual norm 0.0807956 
   37 KSP Residual norm 3.17012e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 2.02226 
      1 KSP Residual norm 0.278904 
      2 KSP Residual norm 0.108493 
   38 KSP Residual norm 2.52737e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.51039 
      1 KSP Residual norm 0.230619 
      2 KSP Residual norm 0.205852 
   39 KSP Residual norm 2.16937e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.41434 
      1 KSP Residual norm 0.202311 
      2 KSP Residual norm 0.156626 
   40 KSP Residual norm 2.1601e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.7393 
      1 KSP Residual norm 0.280888 
      2 KSP Residual norm 0.0871883 
   41 KSP Residual norm 2.12216e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.75397 
      1 KSP Residual norm 0.279145 
      2 KSP Residual norm 0.154592 
   42 KSP Residual norm 2.079e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.12401 
      1 KSP Residual norm 0.155744 
      2 KSP Residual norm 0.131357 
   43 KSP Residual norm 2.05867e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.51986 
      1 KSP Residual norm 0.171536 
      2 KSP Residual norm 0.107456 
   44 KSP Residual norm 2.05802e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.09418 
      1 KSP Residual norm 0.164536 
      2 KSP Residual norm 0.104262 
   45 KSP Residual norm 1.91427e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.25246 
      1 KSP Residual norm 0.162801 
      2 KSP Residual norm 0.133885 
   46 KSP Residual norm 1.55119e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 0.940705 
      1 KSP Residual norm 0.1921 
      2 KSP Residual norm 0.165399 
   47 KSP Residual norm 9.7991e-07 
  Linear solve converged due to CONVERGED_RTOL iterations 47
Update rheology (viscous) [mpoint]: (min,max)_eta 1.00e-03,1.00e+00; log10(max/min) 3.00e+00; cpu time 1.78e-03 (sec)
SNES Object: 8 MPI processes
  type: ksponly
  maximum iterations=50, maximum function evaluations=10000
  tolerances: relative=1e-08, absolute=1e-50, solution=1e-08
  total number of linear solver iterations=47
  total number of function evaluations=2
  norm schedule ALWAYS
  SNESLineSearch Object:   8 MPI processes
    type: basic
    maxstep=1.000000e+08, minlambda=1.000000e-12
    tolerances: relative=1.000000e-08, absolute=1.000000e-15, lambda=1.000000e-08
    maximum iterations=1
  KSP Object:   8 MPI processes
    type: fgmres
      GMRES: restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
      GMRES: happy breakdown tolerance 1e-30
    maximum iterations=10000
    tolerances:  relative=0.001, absolute=1e-50, divergence=10000
    right preconditioning
    using nonzero initial guess
    using UNPRECONDITIONED norm type for convergence test
  PC Object:   8 MPI processes
    type: fieldsplit
      FieldSplit with Schur preconditioner, factorization UPPER
      Preconditioner for the Schur complement formed from A11
      Split info:
      Split number 0 Defined by IS
      Split number 1 Defined by IS
      KSP solver for A00 block
        KSP Object:        (fieldsplit_u_)         8 MPI processes
          type: fgmres
            GMRES: restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
            GMRES: happy breakdown tolerance 1e-30
          maximum iterations=2, initial guess is zero
          tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
          right preconditioning
          using UNPRECONDITIONED norm type for convergence test
        PC Object:        (fieldsplit_u_)         8 MPI processes
          type: mg
            MG: type is MULTIPLICATIVE, levels=3 cycles=v
              Cycles per PCApply=1
              Not using Galerkin computed coarse grid matrices
          Coarse grid solver -- level -------------------------------
            KSP Object:            (fieldsplit_u_mg_coarse_)             8 MPI processes
              type: gmres
                GMRES: restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                GMRES: happy breakdown tolerance 1e-30
              maximum iterations=1, initial guess is zero
              tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
              left preconditioning
              using NONE norm type for convergence test
            PC Object:            (fieldsplit_u_mg_coarse_)             8 MPI processes
              type: bjacobi
                block Jacobi: number of blocks = 8
                Local solve is same for all blocks, in the following KSP and PC objects:
              KSP Object:              (fieldsplit_u_mg_coarse_sub_)               1 MPI processes
                type: preonly
                maximum iterations=10000, initial guess is zero
                tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                left preconditioning
                using NONE norm type for convergence test
              PC Object:              (fieldsplit_u_mg_coarse_sub_)               1 MPI processes
                type: ilu
                  ILU: out-of-place factorization
                  0 levels of fill
                  tolerance for zero pivot 2.22045e-14
                  matrix ordering: natural
                  factor fill ratio given 1, needed 1
                    Factored matrix follows:
                      Mat Object:                       1 MPI processes
                        type: seqaij
                        rows=243, cols=243, bs=3
                        package used to perform factorization: petsc
                        total: nonzeros=28431, allocated nonzeros=28431
                        total number of mallocs used during MatSetValues calls =0
                          using I-node routines: found 54 nodes, limit used is 5
                linear system matrix = precond matrix:
                Mat Object:                 1 MPI processes
                  type: seqaij
                  rows=243, cols=243, bs=3
                  total: nonzeros=28431, allocated nonzeros=28431
                  total number of mallocs used during MatSetValues calls =0
                    using I-node routines: found 54 nodes, limit used is 5
              linear system matrix = precond matrix:
              Mat Object:               8 MPI processes
                type: mpiaij
                rows=1275, cols=1275, bs=3
                total: nonzeros=256671, allocated nonzeros=256671
                total number of mallocs used during MatSetValues calls =0
                  has attached near null space
                  using I-node (on process 0) routines: found 54 nodes, limit used is 5
          Down solver (pre-smoother) on level 1 -------------------------------
            KSP Object:            (fieldsplit_u_mg_levels_1_)             8 MPI processes
              type: gmres
                GMRES: restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                GMRES: happy breakdown tolerance 1e-30
              maximum iterations=2
              tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
              left preconditioning
              using nonzero initial guess
              using NONE norm type for convergence test
            PC Object:            (fieldsplit_u_mg_levels_1_)             8 MPI processes
              type: bjacobi
                block Jacobi: number of blocks = 8
                Local solve is same for all blocks, in the following KSP and PC objects:
              KSP Object:              (fieldsplit_u_mg_levels_1_sub_)               1 MPI processes
                type: preonly
                maximum iterations=10000, initial guess is zero
                tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                left preconditioning
                using NONE norm type for convergence test
              PC Object:              (fieldsplit_u_mg_levels_1_sub_)               1 MPI processes
                type: ilu
                  ILU: out-of-place factorization
                  0 levels of fill
                  tolerance for zero pivot 2.22045e-14
                  matrix ordering: natural
                  factor fill ratio given 1, needed 1
                    Factored matrix follows:
                      Mat Object:                       1 MPI processes
                        type: seqaij
                        rows=1275, cols=1275, bs=3
                        package used to perform factorization: petsc
                        total: nonzeros=256671, allocated nonzeros=256671
                        total number of mallocs used during MatSetValues calls =0
                          using I-node routines: found 425 nodes, limit used is 5
                linear system matrix = precond matrix:
                Mat Object:                (Buu_)                 1 MPI processes
                  type: seqaij
                  rows=1275, cols=1275, bs=3
                  total: nonzeros=256671, allocated nonzeros=256671
                  total number of mallocs used during MatSetValues calls =0
                    using I-node routines: found 425 nodes, limit used is 5
              linear system matrix = precond matrix:
              Mat Object:              (Buu_)               8 MPI processes
                type: mpiaij
                rows=8019, cols=8019, bs=3
                total: nonzeros=2.17655e+06, allocated nonzeros=2.17655e+06
                total number of mallocs used during MatSetValues calls =0
                  has attached near null space
          Up solver (post-smoother) same as down solver (pre-smoother)
          Down solver (pre-smoother) on level 2 -------------------------------
            KSP Object:            (fieldsplit_u_mg_levels_2_)             8 MPI processes
              type: chebyshev
                Chebyshev: eigenvalue estimates:  min = 0.494526, max = 2.71989
                Chebyshev: eigenvalues estimated using gmres with translations  [0 0.2; 0 1.1]
                KSP Object:                (fieldsplit_u_mg_levels_2_esteig_)                 8 MPI processes
                  type: gmres
                    GMRES: restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                    GMRES: happy breakdown tolerance 1e-30
                  maximum iterations=10, initial guess is zero
                  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                  left preconditioning
                  using NONE norm type for convergence test
              maximum iterations=4
              tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
              left preconditioning
              using nonzero initial guess
              using NONE norm type for convergence test
            PC Object:            (fieldsplit_u_mg_levels_2_)             8 MPI processes
              type: jacobi
              linear system matrix = precond matrix:
              Mat Object:              (fieldsplit_u_)               8 MPI processes
                type: shell
                rows=107811, cols=107811, bs=3
          Up solver (post-smoother) same as down solver (pre-smoother)
          linear system matrix = precond matrix:
          Mat Object:          (fieldsplit_u_)           8 MPI processes
            type: shell
            rows=107811, cols=107811, bs=3
      KSP solver for S = A11 - A10 inv(A00) A01 
        KSP Object:        (fieldsplit_p_)         8 MPI processes
          type: preonly
          maximum iterations=10000, initial guess is zero
          tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
          left preconditioning
          using NONE norm type for convergence test
        PC Object:        (fieldsplit_p_)         8 MPI processes
          type: jacobi
          linear system matrix followed by preconditioner matrix:
          Mat Object:          (fieldsplit_p_)           8 MPI processes
            type: schurcomplement
            rows=16384, cols=16384
              Schur complement A11 - A10 inv(A00) A01
              A11
                Mat Object:                (fieldsplit_p_)                 8 MPI processes
                  type: mpisbaij
                  rows=16384, cols=16384, bs=4
                  total: nonzeros=65536, allocated nonzeros=65536
                  total number of mallocs used during MatSetValues calls =0
              A10
                Mat Object:                (Bpu_)                 8 MPI processes
                  type: shell
                  rows=16384, cols=107811
              KSP of A00
                KSP Object:                (fieldsplit_u_)                 8 MPI processes
                  type: fgmres
                    GMRES: restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                    GMRES: happy breakdown tolerance 1e-30
                  maximum iterations=2, initial guess is zero
                  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                  right preconditioning
                  using UNPRECONDITIONED norm type for convergence test
                PC Object:                (fieldsplit_u_)                 8 MPI processes
                  type: mg
                    MG: type is MULTIPLICATIVE, levels=3 cycles=v
                      Cycles per PCApply=1
                      Not using Galerkin computed coarse grid matrices
                  Coarse grid solver -- level -------------------------------
                    KSP Object:                    (fieldsplit_u_mg_coarse_)                     8 MPI processes
                      type: gmres
                        GMRES: restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                        GMRES: happy breakdown tolerance 1e-30
                      maximum iterations=1, initial guess is zero
                      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                      left preconditioning
                      using NONE norm type for convergence test
                    PC Object:                    (fieldsplit_u_mg_coarse_)                     8 MPI processes
                      type: bjacobi
                        block Jacobi: number of blocks = 8
                        Local solve is same for all blocks, in the following KSP and PC objects:
                      KSP Object:                      (fieldsplit_u_mg_coarse_sub_)                       1 MPI processes
                        type: preonly
                        maximum iterations=10000, initial guess is zero
                        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                        left preconditioning
                        using NONE norm type for convergence test
                      PC Object:                      (fieldsplit_u_mg_coarse_sub_)                       1 MPI processes
                        type: ilu
                          ILU: out-of-place factorization
                          0 levels of fill
                          tolerance for zero pivot 2.22045e-14
                          matrix ordering: natural
                          factor fill ratio given 1, needed 1
                            Factored matrix follows:
                              Mat Object:                               1 MPI processes
                                type: seqaij
                                rows=243, cols=243, bs=3
                                package used to perform factorization: petsc
                                total: nonzeros=28431, allocated nonzeros=28431
                                total number of mallocs used during MatSetValues calls =0
                                  using I-node routines: found 54 nodes, limit used is 5
                        linear system matrix = precond matrix:
                        Mat Object:                         1 MPI processes
                          type: seqaij
                          rows=243, cols=243, bs=3
                          total: nonzeros=28431, allocated nonzeros=28431
                          total number of mallocs used during MatSetValues calls =0
                            using I-node routines: found 54 nodes, limit used is 5
                      linear system matrix = precond matrix:
                      Mat Object:                       8 MPI processes
                        type: mpiaij
                        rows=1275, cols=1275, bs=3
                        total: nonzeros=256671, allocated nonzeros=256671
                        total number of mallocs used during MatSetValues calls =0
                          has attached near null space
                          using I-node (on process 0) routines: found 54 nodes, limit used is 5
                  Down solver (pre-smoother) on level 1 -------------------------------
                    KSP Object:                    (fieldsplit_u_mg_levels_1_)                     8 MPI processes
                      type: gmres
                        GMRES: restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                        GMRES: happy breakdown tolerance 1e-30
                      maximum iterations=2
                      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                      left preconditioning
                      using nonzero initial guess
                      using NONE norm type for convergence test
                    PC Object:                    (fieldsplit_u_mg_levels_1_)                     8 MPI processes
                      type: bjacobi
                        block Jacobi: number of blocks = 8
                        Local solve is same for all blocks, in the following KSP and PC objects:
                      KSP Object:                      (fieldsplit_u_mg_levels_1_sub_)                       1 MPI processes
                        type: preonly
                        maximum iterations=10000, initial guess is zero
                        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                        left preconditioning
                        using NONE norm type for convergence test
                      PC Object:                      (fieldsplit_u_mg_levels_1_sub_)                       1 MPI processes
                        type: ilu
                          ILU: out-of-place factorization
                          0 levels of fill
                          tolerance for zero pivot 2.22045e-14
                          matrix ordering: natural
                          factor fill ratio given 1, needed 1
                            Factored matrix follows:
                              Mat Object:                               1 MPI processes
                                type: seqaij
                                rows=1275, cols=1275, bs=3
                                package used to perform factorization: petsc
                                total: nonzeros=256671, allocated nonzeros=256671
                                total number of mallocs used during MatSetValues calls =0
                                  using I-node routines: found 425 nodes, limit used is 5
                        linear system matrix = precond matrix:
                        Mat Object:                        (Buu_)                         1 MPI processes
                          type: seqaij
                          rows=1275, cols=1275, bs=3
                          total: nonzeros=256671, allocated nonzeros=256671
                          total number of mallocs used during MatSetValues calls =0
                            using I-node routines: found 425 nodes, limit used is 5
                      linear system matrix = precond matrix:
                      Mat Object:                      (Buu_)                       8 MPI processes
                        type: mpiaij
                        rows=8019, cols=8019, bs=3
                        total: nonzeros=2.17655e+06, allocated nonzeros=2.17655e+06
                        total number of mallocs used during MatSetValues calls =0
                          has attached near null space
                  Up solver (post-smoother) same as down solver (pre-smoother)
                  Down solver (pre-smoother) on level 2 -------------------------------
                    KSP Object:                    (fieldsplit_u_mg_levels_2_)                     8 MPI processes
                      type: chebyshev
                        Chebyshev: eigenvalue estimates:  min = 0.494526, max = 2.71989
                        Chebyshev: eigenvalues estimated using gmres with translations  [0 0.2; 0 1.1]
                        KSP Object:                        (fieldsplit_u_mg_levels_2_esteig_)                         8 MPI processes
                          type: gmres
                            GMRES: restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                            GMRES: happy breakdown tolerance 1e-30
                          maximum iterations=10, initial guess is zero
                          tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                          left preconditioning
                          using NONE norm type for convergence test
                      maximum iterations=4
                      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                      left preconditioning
                      using nonzero initial guess
                      using NONE norm type for convergence test
                    PC Object:                    (fieldsplit_u_mg_levels_2_)                     8 MPI processes
                      type: jacobi
                      linear system matrix = precond matrix:
                      Mat Object:                      (fieldsplit_u_)                       8 MPI processes
                        type: shell
                        rows=107811, cols=107811, bs=3
                  Up solver (post-smoother) same as down solver (pre-smoother)
                  linear system matrix = precond matrix:
                  Mat Object:                  (fieldsplit_u_)                   8 MPI processes
                    type: shell
                    rows=107811, cols=107811, bs=3
              A01
                Mat Object:                (Bup_)                 8 MPI processes
                  type: shell
                  rows=107811, cols=16384
          Mat Object:          (fieldsplit_p_)           8 MPI processes
            type: mpisbaij
            rows=16384, cols=16384, bs=4
            total: nonzeros=65536, allocated nonzeros=65536
            total number of mallocs used during MatSetValues calls =0
    linear system matrix followed by preconditioner matrix:
    Mat Object:    (stokes_Amf_)     8 MPI processes
      type: shell
      rows=124195, cols=124195
    Mat Object:     8 MPI processes
      type: nest
      rows=124195, cols=124195
        Matrix object: 
          type=nest, rows=2, cols=2 
          MatNest structure: 
          (0,0) : prefix="fieldsplit_u_", type=shell, rows=107811, cols=107811 
          (0,1) : prefix="Bup_", type=shell, rows=107811, cols=16384 
          (1,0) : prefix="Bpu_", type=shell, rows=16384, cols=107811 
          (1,1) : prefix="fieldsplit_p_", type=mpisbaij, rows=16384, cols=16384 
Update rheology (viscous) [mpoint]: (min,max)_eta 1.00e-03,1.00e+00; log10(max/min) 3.00e+00; cpu time 7.98e-04 (sec)
[[ModelOutput_ViscousSinker]]
[[DESIGN FLAW]] pTatin3d_ModelOutput_VelocityPressure_Stokes: require better physics modularity to extract (u,p) <---| (X) 
[[DESIGN FLAW]] pTatinOutputMeshVelocityPressureVTS_v0_binary: only printing P0 component of pressure field 
pTatin3d_ModelOutput_VelocityPressure_Stokes() -> step000000_vp.(pvd,pvts,vts): CPU time 6.34e-02 (sec) 
pTatin3d_ModelOutput_MPntStd() -> step000000_mpoints_std.(pvd,pvtu,vtu): CPU time 6.57e-02 (sec) 
  TimeStep control(StkCourant): | current = 1.0000e+30 : trial = 2.2602e-02 [accepted] | ==>> dt used = 2.2602e-02 |
  TimeStep control(StkSurfaceCourant): | current = 2.2602e-02 : trial = 5.9601e+31 | ==>> dt used = 2.2602e-02 |
  timestep[] dt_courant = 2.2602e-02 
