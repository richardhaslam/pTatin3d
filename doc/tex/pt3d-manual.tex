
\documentclass[paper=a4, fontsize=11pt,twoside]{scrartcl}

% Hacks to make scrartcl work (otherwise these provoke errors)
\makeatletter
\DeclareOldFontCommand{\rm}{\normalfont\rmfamily}{\mathrm}
\DeclareOldFontCommand{\sf}{\normalfont\sffamily}{\mathsf}
\DeclareOldFontCommand{\tt}{\normalfont\ttfamily}{\mathtt}
\DeclareOldFontCommand{\bf}{\normalfont\bfseries}{\mathbf}
\DeclareOldFontCommand{\it}{\normalfont\itshape}{\mathit}
\DeclareOldFontCommand{\sl}{\normalfont\slshape}{\@nomath\sl}
\DeclareOldFontCommand{\sc}{\normalfont\scshape}{\@nomath\sc}
\makeatother

\usepackage[a4paper,pdftex]{geometry}										% A4paper margins
\setlength{\oddsidemargin}{5mm}											% Remove 'twosided' indentation
\setlength{\evensidemargin}{5mm}

\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}

\usepackage[english]{babel}
\usepackage[protrusion=true,expansion=true]{microtype}	
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{graphicx}

\usepackage{fancyhdr}
\usepackage{hypernat}
\usepackage[colorlinks,linkcolor=black,urlcolor=magenta]{hyperref} 
\usepackage[small,nooneline,bf,hang]{caption} 
\parindent0cm                                  
\pagestyle{fancy}
\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{3}

% macros
\newcommand{\ptat}{{{\textsc pTatin3D}}}
\newcommand{\shellcmd}[1]{\\\indent\indent\texttt{\hspace{5mm}\footnotesize #1}\\}
\newcommand{\unix}[1]{\texttt{\footnotesize #1}}

% Definitions for title page
\newcommand{\HRule}[1]{\rule{\linewidth}{#1}} 	% Horizontal rule

\makeatletter							% Title
\def\printtitle{%						
    {
    	\centering \@title\par}
		 	\vspace{10mm}
	\begin{figure} [hbtp]
	\includegraphics[height=0.4\textheight]{figs/ptat3d_front_page_small.pdf}
	\end{figure}
    }


\makeatother									

\makeatletter							% Author
\def\printauthor{%					
    {\centering \large \@author}}				
\makeatother							

%Titlepage
	\title{	\normalsize \textsc{pTatin3D Manual} 	% Subtitle of the document
	\\[2.0cm]									% 2cm spacing
	\HRule{2pt} \\ [0.3cm]						% Upper rule
	\LARGE \textbf{{pTatin3D User Guide}}			% Title
	\HRule{2pt} \\ [0.5cm]						% Lower rule + 0.5cm spacing
	\normalsize \today							% Todays date
	}

\author{
{\normalsize \textsc{Developers}}
\begin{itemize}
\item[-] Dave A. May: Institute of Geophysics, ETH Z{\"u}rich \texttt{(dave.may@erdw.ethz.ch)} 
\item[-] Laetitia Le Pourhiet: UPMC, Univ. Paris 06  \texttt{(laetitia.le\_pourhiet@upmc.fr)}
\item[-] Jed Brown: Argonne National Laboratory, IL USA  \texttt{(jedbrown@mcs.anl.gov)}
\end{itemize}
}



\begin{document}

\thispagestyle{empty}% Remove page numbering on this page

\printtitle% Print the title data as defined above
  	\vfill
\printauthor% Print the author data as defined above


% Begin document


\newpage
\pagenumbering{roman}
\tableofcontents{}

\newpage

\pagenumbering{arabic}

\section{Overview}
{\ptat} provides a suite of functionality to study long-term geodynamic processes related to the dynamics of lithosphere and crust.
At its core, support is provided for solving non-linear, incompressible Stokes flow problems in three-dimensions using a mixed finite element method together with a marker-and-cell method.




% ----------------------------------------------------------------------------------------------------
\newpage
\section{Software Requirements}
\begin{itemize}
	\item \href{http://www.mcs.anl.gov/petsc}{PETSc 3.7}
	\item git (used to obtain the source)
	\item Python (non-essential: required to run the test suite)
	\item \href{www.paraview.org}{ParaView} (non-essential: required for visualisation of simulation output)
	\item libz (non essential: used for output file compression)
\end{itemize}

\section{Obtaining the Source}
The source code for {\ptat} is hosted at \href{https://bitbucket.org}{BitBucket} using the Git version control system.
Currently the repository is private. Upon gaining access to BitBucket, and the {\ptat} repository,
the source code can be obtained using the following git command
\shellcmd{git clone https://USERNAME@bitbucket.org/jedbrown/ptatin3d.git}

\section{Installation}
Upon successful installation of PETSc, switch into the root directory of {\ptat}.
You should provide a file, named \unix{makefile.arch}, which contains specific compiler optimizations for your operating system / compiler.
If you do not provide this, a default, tailored to Mac OS X, will be copied for you.
If you are trying to install {\ptat} on another system,
consult the directory
\unix{config/}
where several different compiler/optimization sets have been provided for you. Provided systems include Linux based cluster, Cray XT5/XE6 and the  IBM BlueGene(L,P,Q). You can use these files as a starting point for you particular system. The recommendation would be to copy the closest architecture file matching your system, e.g. \unix{cp config/machine.arch.XXX  makefile.arch}, then make any necessary changes to the new \unix{makefile.arch} to tailor it for your particular operating system / compiler.
\\[8pt]
%
Once the \unix{makefile.arch} is adjusted for your operating systemtype, type \unix{make} from the root directory of \ptat{}.
\begin{itemize}
	\item Compiling with a particular PETSc build
	\shellcmd{make PETSC\_DIR=/path/to/your/petsc PETSC\_ARCH=your.petsc.build}
	\item Compiling with a particular compilation flags (C and Fortran)
	\shellcmd{make TATIN\_CFLAGS='-O2' TATIN\_FFLAGS='-fast'}
\end{itemize}
%\\[8pt]
%
The following targets for make exist: \unix{all}, \unix{clean}, \unix{drivers}, \unix{test}, \unix{releaseinfo}.
The target \unix{releaseinfo} will automatically update the contents of the file \unix{include/ptatin\_version\_info.h}.
This file is updated to reflect the current revision information associated with the Git master branch. This information is
propagated into log files and the output of {\ptat}.
\\[8pt]
%
The make system will create all objects, libraries and binary files in the following paths:
\begin{enumerate}
\setlength\itemsep{0mm}
	\item[-] \unix{ptatin3d/\$\{PETSC\_ARCH\}/obj}
	\item[-] \unix{ptatin3d/\$\{PETSC\_ARCH\}/lib}
	\item[-] \unix{ptatin3d/\$\{PETSC\_ARCH\}/bin}
\end{enumerate}
The current build system allows you to conveniently compile different {\ptat} using different PETSc builds (e.g. a debug PETSc build versus an optimised build).


% ----------------------------------------------------------------------------------------------------
\newpage
\section{Building Models}

\subsection{Code structure}
The idea we have for building models is that models should be separated as much as possible
from the core functionality of {\ptat}. That is, we want to separate boundary conditions,
mesh geometry, etc as much as possible from the solver used to solve the underlying PDE (Stokes).
\\[8pt]
%%
For this reason, we have adopted a code structure in which models are compiled and combined into
a separate, stand alone library. When using a {\ptat{ driver (i.e which is needed to solve something),
we link the model library against the solver library.
\\[8pt]
%%
Models live under the directory
	\shellcmd{ptatin3d/src/models/}
and the models are compiled into the following static library:
	\shellcmd{ptatin3d/src/models/libptatin3dmodels.a}
An example model called ``template'' is provided in \unix{src/models}.
\\[8pt]
%%	
To add a model, you need to (1) add the directory name containing your model in
	\shellcmd{ptatin3d/src/models/makefile}
under the variable
	\shellcmd{
	TATIN\_MODEL\_DIR = \
        template
	}
and (2), you need to add the filename of \textit{all files within your model directory} into the file
	\shellcmd{ptatin3d/src/models/local.mk}
under the variable
	\shellcmd{
	libptatin3dmodels-y.c = \
        template
	}
%%
Within the directory
	\shellcmd{ptatin3d/src/models/template}
are the files which define any specific data structures needed for the model,
	(\unix{model\_template\_ctx.h})
and a file (\unix{model\_ops\_template.c}) containing the model description.
A ``model'' description in {\ptat} consists of defining the following operations.
\begin{enumerate}
	\item \unix{FP\_pTatinModel\_Initialize} (non essential)
	Initialize any model specific options, and or model specific parameters in your user defined model context.

	\item \unix{FP\_pTatinModel\_ApplyBoundaryCondition}
	Define boundary conditions for the PDE (Stokes, energy).

	\item \unix{FP\_pTatinModel\_ApplyBoundaryConditionMG}
	Define boundary conditions for velocity on each multi-grid level.

	\item \unix{FP\_pTatinModel\_ApplyMaterialBoundaryCondition} (non essential)
	Define the influx of material points if you have prescribed boundary conditions for velocity which are such that $\boldsymbol u \cdot \boldsymbol n < 0$ (e.g. inflow boundary conditions).

	\item \unix{FP\_pTatinModel\_ApplyInitialSolution} (non essential for Stokes)
	Define initial values in the velocity, pressure (non essential) and temperature (essential) vectors. For the Stokes variables ($\boldsymbol u,p$), specifying an initial value may improve convergence of the Stokes solve on the first time step (e.g. by introducing a hydrostatic pressure gradient in the pressure vector).
	
	\item \unix{FP\_pTatinModel\_ApplyInitialMeshGeometry}
	Define the geometry of the mesh. Typically this is done simply by described a hex domain via the PETSc function \unix{DMDASetUniformCoordinates()}.

	\item \unix{FP\_pTatinModel\_ApplyInitialMaterialGeometry}
	Define the initial geometry of the lithology on the markers. (e.g. specify rheology)

	\item \unix{FP\_pTatinModel\_UpdateMeshGeometry} (non essential)
	Define how the mesh should evolve with time. In some models the mesh remains fixed in space through out time, thus this function need not be defined. Other models may wish to deform the mesh with the velocity vector, or may wish to advect the free surface and then apply remeshing within the interior of the domain. Such prescription of ALE mesh movement should be specified here.

	\item \unix{FP\_pTatinModel\_Output} (non essential)
	Specify what mesh fields (e.g. velocity, pressure, temperature) and marker fields will be outputted. Numerous
	methods to output objects from {\ptat} are provided. Any model specific output functions should be called here.

	\item \unix{FP\_pTatinModel\_Destroy} 
	Upon completion of a {\ptat} job, this function will be called to release the memory allocated which is associated 
	with this particular model. If no data structures were defined, then this function does not need to be defined.
\end{enumerate}

Following the definition of the above functions, to complete the model definition we have to perform the following steps :
\begin{enumerate}
	\item \unix{pTatinModelCreate()}
Calling this creates a little structure to hold your function pointers and other model related information.

	\item \unix{pTatinModelSetName(...,MODELNAME);}
The variable in \unix{MODELNAME}, will used as the command line argument used to select the model
	\unix{-ptatin\_model MODELNAME}

	\item \unix{pTatinModelSetUserData()}
Set any data structures required by the model.

	\item Assign the function pointers. This is done via \unix{pTatinModelSetFunctionPointer()}.
The second argument indicates which function pointer is used. These are defined via
\unix{typedef enum \{ \} pTatinModelOperation;}
and are declared in 
\unix{ptatin\_models.h}
To assign the operations which your model will perform, a helper function (\unix{pTatinModelSetFunctionPointer()}) is provided.
Example usage:
\shellcmd{pTatinModelSetFunctionPointer(...,PTATIN\_MODEL\_APPLY\_BC,my\_apply\_bc\_function);}
\shellcmd{pTatinModelSetFunctionPointer(...,PTATIN\_MODEL\_APPLY\_INIT\_MAT\_GEOM,my\_apply\_init\_material\_geom\_function);} %line overrun

	\item Register the model via the call
\unix{pTatinModelRegister()}
This will add your model definition into a list which {\ptat} will have access to.
Steps 1-5 are provided within the function \unix{pTatinModelRegister\_Template()}.

	\item Finally, you need to edit
	\unix{ptatin\_models\_reg.c}
and add the function call to register your model.
This should be done within
		\unix{pTatinModelRegisterAll()}
as you'll note, you will see the ``template'' model registration function 	
		\unix{pTatinModelRegister\_Template();}

So you don't have the compiler warning about ``implicitly defined function'', simply add the prototype as an \unix{extern}, e.g.
	\shellcmd{extern PetscErrorCode pTatinModelRegister\_Template(void);}
\end{enumerate}

\subsection{Compiling your model}
There is also a makefile within each model in the directory.
For example,
	\shellcmd{ptatin3d/src/models/template/makefile}
In general, the makefiles for each different model will all be very similar.
It is recommended to use the model makefile identified above as the basis of a makefile for any new model.
For most new models, one would simply have to list the C files used in defining the model in the variable
\newline
\unix{
\#\# List source files here \newline
MODEL\_SRC = \newline
        template.c \newline
}
Models must be compiled at the root level of the code tree.
That is, to make a model, you must run ``make'' from 
	\unix{ptatin3d}
You \textit{should not} run make from within the model directory
	\unix{ptatin3d/src/models}
or
	\unix{ptatin3d/src}
or
	\unix{ptatin3d/src/models/template}.
%If you only want to compile the models within
%	\unix{ptatin3d/src/models},
%then execute the following command
%	\unix{make models}
%from the directory
%	\unix{ptatin3d/src}.

\subsection{Implementing Dirichlet boundary conditions}
{\ptat} uses ``iterators'' to assist you in defining boundary and initial conditions. Iterators allow you to apply user defined functions to set boundary and initial conditions into the data structures and solution vectors used by {\ptat} without requiring the user to see the underlying data structures used, or understand parallelism.
\\[8pt]
%%
To apply boundary conditions, the main function used is:
\shellcmd{PetscErrorCode DMDABCListTraverse3d( \newline
 BCList list, \\
  DM da, \\
  DMDABCListConstraintLoc doflocation, \\
  PetscInt dof\_idx, \\
  PetscBool (*evaluate\_boundary\_condition)(PetscScalar*,PetscScalar*,void*), \\
  void *ctx)} 
which is declared in \unix{dmda\_bcs.c}, where the input arguments are identified as;
\begin{itemize}
%%
\item[] \unix{BCList list} - data structure used to store boundary conditions.
%%
\item[] \unix{DM da} - data structure used to represent the mesh and quantity (e.g. velocity).
%%
\item[] \unix{DMDABCListConstraintLoc doflocation} - index identifying which region of the mesh you wish to attach a boundary condition to. The value used here should be one of 
	\{ \unix{DMDABCList\_INTERIOR\_LOC, 
	DMDABCList\_IMIN\_LOC,
	DMDABCList\_IMAX\_LOC,
	DMDABCList\_JMIN\_LOC,
	DMDABCList\_JMAX\_LOC,
	DMDABCList\_KMIN\_LOC,
	DMDABCList\_KMAX\_LOC} \}.
Refer to Fig.~\ref{fig:domain_coord} for the geometric interpretation of these names.
%%
\item[] \unix{PetscInt dof\_idx} - integer identifying which degree of freedom the constraint should be applied to (.e.g $v_x$ would correspond to 0, $v_y$ would correspond to 1).
%%
\item[] \unix{PetscBool (*evaluate\_boundary\_condition)(PetscScalar*,PetscScalar*,void*)} - the user provided function which defines the boundary condition.
%%
\item[] \unix{void *ctx} - any data structure, or parameters which are required by the user provided function to evaluate the boundary condition.
%%
\end{itemize}

The calling sequence for \unix{evaluate\_boundary\_condition} is,
\shellcmd{PetscBool evaluate\_boundary\_condition( PetscScalar position[], PetscScalar *value, void *ctx )}
where the arguments are;
\begin{itemize}
\item[] \unix{PetscScalar position[]} - coordinates in space where the boundary condition is evaluated.
\item[] \unix{void *ctx} - user defined data require to evaluate the boundary condition.
\item[] \unix{PetscScalar *value} - the actual value of the boundary condition [OUTPUT].
\item[] \unix{PetscBool} - the function must return \{ \unix{PETSC\_TRUE, PETSC\_FALSE} \} to indicate whether the boundary condition should be applied at this location [OUTPUT].
\end{itemize}
For a simple example of a user defined boundary condition function, refer to 
\shellcmd{PetscBool BCListEvaluator\_constant(PetscScalar position[],PetscScalar *value,void *ctx)}
in file \unix{dmda\_bcs.c}.
\\[8pt]
%%	
A standard set of velocity Dirichlet boundary conditions for Stokes are defined in \unix{ptatin\_std\_dirichlet\_boundary\_conditions.c}. Please refer to  \unix{ptatin\_std\_dirichlet\_boundary\_conditions.h} for a summary of the functions available.




Notes: 
\begin{enumerate}
\item Boundary conditions can be applied within the interior of the domain using the value \unix{DMDABCList\_INTERIOR\_LOC}.
\item Boundary conditions do not have to be applied along an entire face. Prescribing boundary conditions internal to a face is controlled by whether your user provided function returns  \unix{PETSC\_TRUE} or \unix{PETSC\_FALSE}. For example, if the bc evaluator was called with the argument \unix{DMDABCList\_IMIN\_LOC} we will traverse along the bottom boundary. The user defined function could return  \unix{PETSC\_TRUE} if $x < 0.5$ and \unix{PETSC\_FALSE} otherwise. When \unix{PETSC\_FALSE} is returned the parameter \unix{PetscScalar *value} is ignored.
\item When defining time dependent boundary conditions, the variable time should be included within the user defined structured (\unix{void *ctx}) which is passed into \unix{DMDABCListTraverse3d()}.
\end{enumerate}

\subsection{Implementing initial conditions}
Iterators are provided to allow to define the contents of PETSc vectors via user defined functions. This is particularly useful to define initial conditions, e.g. for the temperature field. The procedure is similar to that used to define boundary conditions described above. The two iterators used to fill in values within a vector are \unix{DMDAVecTraverse3d} and \unix{DMDAVecTraverseIJK} which are declared in \unix{dmda\_iterator.c}. Below we describe the calling pattern of these two functions.
\\[8pt]
%%	
The standard vector iterator is given by
\shellcmd{PetscErrorCode DMDAVecTraverse3d( \newline
  DM da, \\
  Vec X, \\
  PetscInt dof\_idx, \\
  PetscBool (*evaluate\_function)(PetscScalar*,PetscScalar*,void*), \\
  void *ctx),} 
 where the input arguments are identified as;
\begin{itemize}
%%
\item[] \unix{DM da} - data structure used to represent the mesh and quantity (e.g. velocity).
%%
\item[] \unix{Vec X} - PETSc vector into which you want to insert values (e.g. velocity).
%%
\item[] \unix{PetscInt dof\_idx} - integer identifying which degree of freedom the constraint should be applied to (.e.g $v_x$ would correspond to 0, $v_y$ would correspond to 1).
%%
\item[] \unix{PetscBool (*evaluate\_function)(PetscScalar*,PetscScalar*,void*)} - the user provided function.
%%
\item[] \unix{void *ctx} - any data structure or parameters which are required to evaluate the the user provided function.
%%
\end{itemize}
The calling sequence for \unix{evaluate\_function} is,
\shellcmd{PetscBool evaluate\_function( PetscScalar position[], PetscScalar *value, void *ctx )}
where the arguments are
\begin{itemize}
\item[] \unix{PetscScalar position[]} - coordinates in space where the user defined function will be evaluated.
\item[] \unix{void *ctx} - user defined data require to evaluate the boundary condition.
\item[] \unix{PetscScalar *value} - the result of the user provided function [OUTPUT].
\item[] \unix{PetscBool} - the function must return \{ \unix{PETSC\_TRUE, PETSC\_FALSE} \} to indicate whether the value should be inserted into the vector [OUTPUT].
\end{itemize}
%
%
%\shellcmd{PetscErrorCode DMDAVecTraverseIJK(DM da,Vec X,PetscInt dof\_idx,PetscBool (*evaluate\_functionIJK)(PetscScalar*,PetscInt*,PetscInt*,PetscScalar*,void*),void *ctx)}
%
%

The standard $ijk$ vector iterator is given by
\shellcmd{PetscErrorCode DMDAVecTraverseIJK( \newline
  DM da, \\
  Vec X, \\
  PetscInt dof\_idx, \\
  PetscBool (*evaluate\_functionIJK)(PetscScalar*,PetscInt*,PetscInt*,PetscScalar*,void*), \\
  void *ctx),} 
 where the input arguments are identified as;
\begin{itemize}
%%
\item[] \unix{DM da} - data structure used to represent the mesh and quantity (e.g. velocity).
%%
\item[] \unix{Vec X} - PETSc vector into which you want to insert values (e.g. velocity).
%%
\item[] \unix{PetscInt dof\_idx} - integer identifying which degree of freedom the constraint should be applied to (.e.g $v_x$ would correspond to 0, $v_y$ would correspond to 1).
%%
\item[] \unix{PetscBool (*evaluate\_functionIJK)(PetscScalar*,PetscInt*,PetscInt*,PetscScalar*,void*)} - the user provided function.
%%
\item[] \unix{void *ctx} - any data structure or parameters which are required to evaluate the the user provided function.
%%
\end{itemize}
The calling sequence for \unix{evaluate\_functionIJK} is,
\shellcmd{PetscBool evaluate\_functionIJK( PetscScalar position[], PetscInt global\_index[], PetscInt local\_index[], PetscScalar *value, void *ctx )}
where the arguments are
\begin{itemize}
\item[] \unix{PetscScalar position[]} - coordinates in space where the user defined function will be evaluated.
\item[] \unix{PetscInt global\_index[]} - $i,j,k$ values of the node in the global numbering system.
\item[] \unix{PetscInt local\_index[]} - $i,j,k$ values of the node in the local numbering system.
\item[] \unix{void *ctx} - user defined data require to evaluate the boundary condition.
\item[] \unix{PetscScalar *value} - the result of the user provided function [OUTPUT].
\item[] \unix{PetscBool} - the function must return \{ \unix{PETSC\_TRUE, PETSC\_FALSE} \} to indicate whether the value should be inserted into the vector [OUTPUT].
\end{itemize}




% ----------------------------------------------------------------------------------------------------
\newpage
\section{Drivers}
{\ptat} in itself is a library which aims to enable users to build specific executables to solve a particular class of problems in lithospheric dynamics.
Executables which utilize \unix{libptatin3d} are referred to as drivers.
For standard problems, e.g. time dependent linear (or non-linear) Stokes problems with optionally thermal coupling, we provide several default drivers.
The most important (i.e. useful) of these are summarized below.

\begin{enumerate}
	\item \unix{ptatin\_driver\_ic.app}
	Specifically designed to confirm that the model is defined in accordance with what the developer had in mind. This driver does not solve any equations or perform any time stepping. Rather it only loads the model, loads the initial conditions and boundary conditions and calls the model output function.
	
	\item \unix{ptatin\_driver\_energy.app}
	A basic driver used for testing the advection diffusion solver. It does not solve Stokes.
	
	\item \unix{ptatin\_driver\_linear\_ts.app}
	A driver for solving time dependent, \emph{linear} Stokes flow problems. Checkpointing is enabled.

	\item \unix{ptatin\_driver\_nonlinear\_ts.app}
	A driver for solving time dependent, \emph{non-linear} Stokes flow problems. The energy equation may be optionally activated within this model.
\end{enumerate}

\subsection{Options for standard drivers}
\begin{itemize}
	\item \unix{-nsteps}: Number of time steps to perform.
	\item \unix{-output\_frequency}: Interval to output data.
	\item \unix{-output\_path}: Directory where output should be written to.
	\item \unix{-dt\_max}: Upper bound on the time step permitted.
	\item \unix{-dt\_min}: Lower bound on the time step permitted.
	\item \unix{-constant\_dt}: Specifies a constant time step size which should be used.
	\item \unix{-time\_max}: Maximum time (in model units) which simulation should perform.
	%%
	\item \unix{-checkpoint\_every}: Indicates interval which check point files with a non-unique name should be written.
	\item \unix{-checkpoint\_every\_nsteps}: Indicates interval which check point files with a unique name (associated with the current time step) should be written.
	\item \unix{-checkpoint\_every\_ncpumins}: Indicates interval in terms of CPU time with which a check point files with a unique name (associated with the current time step) should be written.
\end{itemize}

% ----------------------------------------------------------------------------------------------------
\newpage
\section{Example models and tests}
A small test suite is provided with the source code for {\ptat}. These are used to ensure that core functionality is producing the correct results. The tests can be found in the directory \unix{test/}. To execute the tests, ensure that you have Python and numpy and execute \unix{make test}(a quick subset of tests) or \unix{make testall}(all tests) from the \ptat{} root directory.

Numerous example models are provided with the source distribution. These serve as a starting point to generate your own models. The most simple model is ``viscous\_sinker'', with source files living in \unix{src/models/viscous\_sinker}. Several example options files are provided in \unix{src/models/viscous\_sinker/testjobs}. We summarize the examples below
\begin{enumerate}
	\item ex\_sinker.opts: Defines a dense, viscous rectangular block in a unit cube domain. The rectangular block sinks due to the buoyancy difference between the block and background material. ``Free slip'' boundaries (zero normal velocity, zero tangential stress) are applied along all boundaries. The model is time dependent with a linear viscous rheology, thus the driver \unix{ptatin\_driver\_linear\_ts.app} should be used.

	\item sinker-ex2.opts: Defines a buoyant spherical viscous inclusion in a domain $\Omega = [0,1] \times [0,\tfrac{1}{4}] \times [0,1]$. Boundary conditions on the upper surface $\sigma_{ij}n_j=0$ (``free surface'') and ``free slip'' on all other sides. The upper surface is initialized to have a small amount of positive topography. Only a single time step is performed. The driver \unix{ptatin\_driver\_linear\_ts.app} should be used. This option file illustrates how flexible multi-grid coarsening and coarse grid operators can be used. See Sec.~\ref{sec:configsolvers} for further details.

	\item sinker-mfscaling.opts: Defines a dense, viscous spherical inclusion in a unit cube domain. A``free surface'' upper boundary is used and all other boundaries are ``free slip''. The upper surface is initially flat. Only a single time step is performed. The driver \unix{ptatin\_driver\_linear\_ts.app} should be used. This option file is used for strong scaling experiments.
\end{enumerate}

% ----------------------------------------------------------------------------------------------------
\newpage
\section{Generated output}
All output generated via {\ptat} will be written in the directory specified via the option \unix{-output\_path}. If this option isn't provided, a default directory called ``output'' will be generated.

\subsection{Diagnostics}
The time dependent drivers will generate a log file (ptatin.log-XXX) and a file containing all options (inputs) used for the current simulation (ptatin.options-XXX). Here, ``XXX'' will be replaced with a year/date/time stamp (e.g. 2013.10.31\_11:00:54) corresponding to when the executable was launched. The time stamp ensures unique files will be give to each model run, thus enabling them to reproduced at a later date. Log files contain information regarding the mesh resolution, memory usage, CPU times associated with each solve and diagnostics about the solver (number of iterations, residuals, etc) and the time step, current time associated with the simulation. The options file generated can be used to re-run {\ptat} models simply via executing \unix{./ptatin\_DRIVER.app -options\_file ptatin.options-XXX}.

\subsection{Simulation results}
{\ptat} generates output in formats which can be visualized via the open source tool ParaView (\url{www.paraview.org}).
%A complete description of ParaView formats can be found here (\url{TODO}).
A comprehensive ParaView tutorial can be found here (\url{http://www.paraview.org/Wiki/The\_ParaView\_Tutorial}). 
Additional tutorial information can be found here:
\begin{tiny}
\begin{itemize}
	\item[-] \url{http://www.bu.edu/tech/about/research/training/online-tutorials/paraview/}
	\item[-] \url{http://www.cac.cornell.edu/education/Training/data10/VisualizationWithParaView.pdf}
\end{itemize}
\end{tiny}

A summary of file extensions commonly produced by {\ptat} is provided below: \newline
{\bf *.pvd}: Contains time dependent datasets.  \newline
{\bf *.pvts}: Contains mesh data (nodal fields and cell fields) for the complete mesh.  \newline
{\bf *.vts}: Contains mesh data (nodal fields and cell fields) for each sub-domain used in the parallel computation.  \newline
{\bf *.pvtu}: Contains particle data for the complete set of material points.  \newline
{\bf *.vtu}: Contains particle data for each sub-domain used in the parallel computation. \newline

Depending on the particular output functions called in your model, numerous fields are generated during a {\ptat} simulation.
The term ``sub-domain'' used above refers to the piece of the mesh assigned to each core. In the mode of parallelism used in {\ptat}, there is always one sub-domain per core.

\subsubsection{Mesh data}
{\bf Velocity and pressure:} Both the velocity and pressure fields are contained in the following set of files: \\
\unix{timeseries\_vp.pvd}, \\
\unix{step000000\_vp.pvts}, \\
\unix{step000000\_vp-subdomain00000.vts}.\\
Floating point numbers are stored as double precision. All data is stored in a binary format. The function \\
\unix{pTatin3d\_ModelOutput\_VelocityPressure\_Stokes()} \\
generates these data files.
\\[8pt]
%%	
{\bf Velocity:} The velocity field is contained in the following set of files: \\
\unix{timeseries\_v.pvd}, \\
\unix{step000000\_v.pvts}, \\
\unix{step000000\_v-subdomain00000.vts}. \\
Floating point numbers are stored as single precision. All data is stored in a binary format.
The function \\
\unix{pTatin3d\_ModelOutputLite\_Velocity\_Stokes()} \\
generates these data files.
\\[8pt]
%%	
{\bf Temperature:} The temperature field is contained in the following set of files: \\
\unix{timeseries\_T.pvd}, \\
\unix{step000000\_T.pvts}, \\
\unix{step000000\_T-subdomain00000.vts}.\\
Floating point numbers are stored as single precision. All data is stored in a binary format.
The function \\
\unix{pTatin3d\_ModelOutput\_Temperature\_Stokes()} \\
generates these data files.


\subsubsection{Material point data}
Material point data can be represented in two distinct manners: 
\begin{enumerate}
	\item As a point cloud, e.g. a set of unconnected points $x_p$ with a set of material point properties (e.g $\eta_p$, $\rho_p$, ...) defined at each point coordinate $x_p$. 
	We refer to this as a ``point-wise'' representation.
	\item As cell constant values defined on the hex mesh (or sub-mesh hex) used to approximate the velocity field. 
	We refer to this as a ``cell-wise'' representation.
\end{enumerate}
Each form of representing material point data has several advantages and disadvantages.
The point-wise representation of material point data has the following limitations: (i) the point-wise representation requires a large amount of storage, and (ii) investigation of the 3D data set is limited as operations such as iso-surfaces, clipping, etc cannot be defined on point clouds. Furthermore, points do not represent a finite volume, thus they do not define a surface. Hence no lighting/shading can be applied, thereby limiting the depth perception of the material point data set. However, the point-wise representation is useful for debugging model setups. The cell-wise representation of material point data overcomes both of the two limitations of the point-wise representation. However, to define the material properties on each cell we have to introduce an interpolation (or projection) between the data defined on the material points and the mesh. This means that the result stored in the cell-wise data representation is only an \emph{approximation} of the true material point data. Cell-wise representation is useful for high-resolution simulations and production runs in which the correctness of model setups has been verified.
\\[8pt]
%%	
{\bf Point-wise representation:} Material point data fields associated with the basic marker type (\unix{MPField\_Std}) are contained in the following set of files: \\
\unix{timeseries\_mpoints\_std.pvd}, \\
\unix{step000000\_mpoints\_std.pvtu}, \\
\unix{step000000\_mpoints\_std-subdomain00000.vtu}. \\
Floating point numbers are stored in the precision associated with each material point data field. All data is stored in a binary format. The function \\
\unix{pTatin3d\_ModelOutput\_MPntStd()} \\
generates these data files.
\\[8pt]
%%	
A more flexible mechanism to output material point data associated with multiple marker types is provided by the function \unix{SwarmViewGeneric\_ParaView()}. Note that this function will not generate for you a *.pvd file.  All data is stored in a binary format. Marker types which can be output in these files are identified via the the following names: \\
\unix{MPField\_Std, MPField\_Stokes, MPField\_StokesPl, MPField\_Energy}.
%%
\\[8pt]
%%	
Marker data can alternatively be represented on the mesh used for computing the velocity/pressure solution. 
In this case, the material point data field is averaged over each element and represented as a constant value within each element.
\\[8pt]
%%	
{\bf Cell-wise representation:} Material point data fields which are represented on the mesh are contained in the following set of files:\\
\unix{timeseries\_mpoints\_cell.pvd}, \\
\unix{step000000\_mpoints\_cell.pvts}, \\
\unix{step000000\_mpoints\_cell-subdomain00000.vts}. \\
Floating point numbers are stored in the precision associated with each material point data field. All data is stored in a binary format.
The function \\
	\unix{pTatin3d\_ModelOutput\_MarkerCellFields()} \\
generates these data files. This function is flexible and permits specific marker fields to be output into the file. Marker fields which can be output in these files are identified via the the following names:
\unix{MPV\_region, MPV\_viscosity, MPV\_density, MPV\_plastic\_strain, MPV\_yield\_indicator, MPV\_diffusivity, MPV\_heat\_source}.

% ----------------------------------------------------------------------------------------------------
\newpage
\section{Nonlinear and Linear Solvers}

The discrete linear Stokes problem can be expressed as
\begin{equation}
	\begin{bmatrix}
	A 	&B	\\
	B^T	&0
	\end{bmatrix}
	\begin{bmatrix}
	u \\
	p
	\end{bmatrix}
	=
	\begin{bmatrix}
	f	\\
	h
	\end{bmatrix}
	\label{eq:discretelinearstokes}
\end{equation}
We solve Eq.~\eqref{eq:discretelinearstokes} via a right preconditioned Krylov method using an Elman-Wathan style upper block triangular preconditioner. The preconditioned system is thus
\begin{equation}
	\begin{bmatrix}
	A 	&B	\\
	B^T	&0
	\end{bmatrix}
	\begin{bmatrix}
	A 	&B	\\
	0	&-S
	\end{bmatrix}^{-1}
	\begin{bmatrix}
	\tilde{u} \\
	\tilde{p}
	\end{bmatrix}
	=
	\begin{bmatrix}
	f	\\
	h
	\end{bmatrix}
	\label{eq:discretelinearstokes_pc}
\end{equation}
where
\begin{equation}
	\begin{bmatrix}
	\tilde{u} \\
	\tilde{p}
	\end{bmatrix}
	=
	\begin{bmatrix}
	A 	&B	\\
	0	&-S
	\end{bmatrix}
	\begin{bmatrix}
	u \\
	p
	\end{bmatrix}
\end{equation}
The upper block triangular preconditioner is only required to be applied to vectors. Rather than assembling the inverse and multiplying with an arbitrary vector $(x_1,x_2)$, we re-state the problem in the following manner
\begin{equation}
	\begin{bmatrix}
	A 	&B	\\
	0	&-S
	\end{bmatrix}^{-1}
	\begin{bmatrix}
	{x_1} \\
	{x_2}
	\end{bmatrix}
	=
	\begin{bmatrix}
	y_1	\\
	y_2
	\end{bmatrix}
%
\quad
	\longrightarrow
\quad
	\text{Solve }
	\begin{bmatrix}
	A 	&B	\\
	0	&-S
	\end{bmatrix}
	\begin{bmatrix}
	y_1	\\
	y_2
	\end{bmatrix}
	=
	\begin{bmatrix}
	{x_1} \\
	{x_2}
	\end{bmatrix}
	\medspace \text{for } y_1, y_2	
\end{equation}
The solve involving the upper triangular system is defined in two steps:
\begin{enumerate}
	\item Solve $-S \, y_2 = x_2$ for $y_2$
	\item Solve $A \, y_1 = x_1 - B \, y_2$ for $y_1$
\end{enumerate}

The iterative solution strategy for the Stokes flow problem is a nested procedure, containing many sub-solves. 
The nested form of the method used is depicted in Fig.~\ref{fig:solver_hierarchy}.
We will refer to the outer most solver (i.e. that for Eq.~\eqref{eq:discretelinearstokes_pc}) as the ``Stokes solve''. Any solver associated with the $A$ matrix will be verified to as a ``Viscous block'' solve. Any solver related to $S$ will be referred to as a ``Pressure Schur complement'' solve.
\begin{figure} [hbtp]
\center
\includegraphics[height=0.6\textheight]{figs/pt3d_solve_hierarchy.pdf} %note : typo 'comaring'
\caption[\itshape ]
{\itshape Solver hierarchy}
\label{fig:solver_hierarchy}
\end{figure}
In the following section we detail how to configure the solver applied to Eq.~\eqref{eq:discretelinearstokes_pc} to obtain $(u,p)$, and how to configure the solvers in steps 1 ({\tt -fieldsplit\_u\_ksp}) and 2 ({\tt -fieldsplit\_p\_ksp}) of applying the upper triangular preconditioner.


\subsection{Configuring solvers} \label{sec:configsolvers}

\subsubsection{Non-linear Stokes solver}
Non-linear methods adopt the following sequence
\begin{enumerate}
	\item Given $x_0$, set $k = 0$. Compute $\| F (x_0) \|$
	\item Solve $J(x_k) \delta x = -F(x_k)$
	\item $x_{k+1} = x_k + \delta x$
	\item Compute $\| F (x_{k+1}) \|$
	\item $k = k + 1$, goto step 2 until converged.
\end{enumerate}
The non-linear iteration sequence may be terminated for several different  reasons. The reasons and the controlling parameters (modifiable from the command line are provided below):\newline
\unix{-snes\_rtol 1.0e-3} : stops if $\| F (x_{k+1}) \| \le 10^{-3} \| F (x_0) \|$ \newline
\unix{-snes\_atol 1.0e-3} : stops if $\| F (x_{k+1}) \| \le 10^{-3}$  \newline 
\unix{-snes\_stol 1.0e-3} : stops if $\| \delta x \| \le 10^{-3} \| x_{k+1} \|$ \newline
\unix{-snes\_max\_it 30} : stops if 30 non-linear iterations occur \newline
\unix{-snes\_max\_funcs 30} : stops if 30 non-linear residual evaluations occur \newline

\subsubsection{Linearized Stokes solve}
\unix{-ksp\_rtol 1.0e-3} : stops if $\| J_k \delta x + F_k \| \le 10^{-3} \| J_0 \delta x + F_0 \|$ \newline
\unix{-ksp\_atol 1.0e-3} : stops if $\| J_k \delta x + F_k  \| \le 10^{-3}$  \newline 
\unix{-ksp\_max\_it 30} : stops if 30 Stokes iterations occur \newline

\subsubsection{Viscous block solve}
\unix{-fieldsplit\_u\_ksp\_rtol 1.0e-3} : stops if $\| A u_k - b \| \le 10^{-3} \| A u_k - b \|$ \newline
\unix{-fieldsplit\_u\_ksp\_atol 1.0e-3} : stops if $\| A u_k - b  \| \le 10^{-3}$  \newline 
\unix{-fieldsplit\_u\_ksp\_max\_it 30} : stops if 30 viscous block iterations occur \newline



\subsection{Multi-grid sequencing}
The number of multi-grid levels is set via the option:
\shellcmd{-dau\_nlevels X}
If $X = 3$, this will imply that the mesh specified via \unix{-mx 16 -my 16 -mz 16} will be coarsened twice, resulting (by default) in a mesh sequence with the number of elements $M_x = 16$ (fine), $M_x = 8$ and $M_x = 4$ (coarse). For clarity, level 0 is designated as the ``coarsest'' level, and level $X-1$ is designated as the ``finest'' level.
\\[8pt]
%%	
Meshes can be coarsened by a constant factor on each level, or one can control the coarsening factor level-by level.
In both cases, the coarsening applied is dependent on the coordinate direction $(i,j,k)$. 
To define constant coarsening on every level, but varying in each direction, use the options 
\unix{-da\_refine\_x}, \unix{-da\_refine\_y}, and \unix{-da\_refine\_z}, for example
\shellcmd{-da\_refine\_x 4\\
\indent\indent\texttt{\hspace{5mm}}-da\_refine\_y 2 \\
\indent\indent\texttt{\hspace{5mm}}-da\_refine\_z 1}
Note that different coarsening can be specified in each direction - here denoted via \unix{x},\unix{y},\unix{z} in the refinement command line option.
Also note that a refinement factor of 1 implies that mesh will not be coarsened in that direction. 
\\[8pt]
%%	
To define level specific coarsening, use the options 
\unix{-stk\_velocity\_da\_refine\_hierarchy\_x}, 
\unix{-stk\_velocity\_da\_refine\_hierarchy\_y}, and
\unix{-stk\_velocity\_da\_refine\_hierarchy\_z}, for example
\shellcmd{-stk\_velocity\_da\_refine\_hierarchy\_x 4,1}
The integer parameters are listed from coarse to fine levels (left to right). 
The above options will result result in the following hierarchy in the $x$ direction (from coarse to fine), $M_x = \{ 4, 16, 16 \}$.
Note that if you have $X$ levels, you are only required to specify $X-1$ values to define the coarsening hierarchy. 
\\[8pt]
%%	
Note that when using geometric multi-grid, all levels are spatial decomposed across all cores. Furthermore, it is required that every core contains at least one element. Thus, the total number of cores used to run a {\ptat} job must not exceed the number of elements within the coarse grid. If you require additional levels in your multi-grid hierarchy, we recommend you use an algebraic multi-grid preconditioner as your coarse grid preconditioner.

\subsection{Coarse grid operators}
Within {\ptat}, the style of coarse grid operator can be configured at run time. 
We provide support to enable four types of coarse grid operator; $\{$ assembled re-discretised = 0, matrix-free re-discretised = 1, Galerkin = 2, matrix-free Galerkin = 3 $\}$. 
Re-discretised operators imply that the viscosity is projected through the mesh hierarchy and the finite element operators are re-evaluated using the standard FE weak form defined on the fine level. 
Galerkin coarsening implies that the coarse grid operator at level $k$ $A_k$ is defined via $A_k = R^T A_{k+1} R$, where $R$ is the restriction operator for level $k+1$ to $k$. 
To define the triple matrix product required by the a Galerkin operator definition on level $k$, it is require that the operator on level $k+1$ be assembled, i.e. be of either type $\{$ assembled re-discretised = 0, Galerkin = 2 $\}$. Configuration of the coarse grid operator type is define via the option \shellcmd{-A11\_operator\_type}, for example
\shellcmd{-A11\_operator\_type 2,2,0,1}
where valid indices are $\{ 0, 1, 2, 3 \}$ and operators on each level are specified from coarse to fine (left to right).

\subsection{Multi-grid performance profiling}
To activate monitors which report CPU times on each multi-grid level, one must specify the following options
\shellcmd{-dau\_nlevels X -fieldsplit\_u\_pc\_type mg -fieldsplit\_u\_pc\_mg\_levels X -fieldsplit\_u\_pc\_mg\_log -log\_summary}
Note that if the values for \unix{-dau\_nlevels X} and \unix{-fieldsplit\_u\_pc\_mg\_levels X} do not match, logging will not be performed.


\subsection{Understanding residuals}

When solving linear problems, two types of residuals are important to monitor for both accuracy of the Stokes problem and efficiency of the solver.
By default, all drivers report the Stokes residuals for the individual components of the residual associated with the $u,v,w$-momentum equations and the continuity equation. The complete residual associated with the residual of the full vector $(u,v,w,p)$ can be monitored using the flag
\shellcmd{-ksp\_monitor}
\\[8pt]
%%	
The residuals output to the screen will typically look like the following:
\begingroup
    \fontsize{6.5pt}{6.5pt}\selectfont
	\begin{verbatim}
  0 KSP Component U,V,W,P residual norm [ 0.000000000000e+00, 2.307691625955e-03, 0.000000000000e+00, 0.000000000000e+00 ]
        Residual norms for fieldsplit_u_ solve.
        0 KSP Residual norm 1.000000000000e+00 
        1 KSP Residual norm 9.566218971472e-01 
        2 KSP Residual norm 6.120179419948e-01 
        3 KSP Residual norm 2.693954462485e-01 
        4 KSP Residual norm 7.305148723409e-02 
        5 KSP Residual norm 2.484961341074e-02 
        6 KSP Residual norm 7.872576358279e-03 
  1 KSP Component U,V,W,P residual norm [ 1.931850895378e-07, 2.249650371310e-03, 1.931669353791e-07, 3.613480131486e-04 ]
        Residual norms for fieldsplit_u_ solve.
        0 KSP Residual norm 9.383540923375e-01 
        1 KSP Residual norm 2.353199854638e-01 
        2 KSP Residual norm 6.233285019473e-02 
        3 KSP Residual norm 2.466199083002e-02 
        4 KSP Residual norm 6.082964600394e-03 
  2 KSP Component U,V,W,P residual norm [ 2.862389390637e-05, 9.461315464033e-04, 2.873233657833e-05, 1.128984116051e-03 ]
        Residual norms for fieldsplit_u_ solve.
        0 KSP Residual norm 1.461862868221e+01 
        1 KSP Residual norm 6.787824394358e-01 
        2 KSP Residual norm 1.306125161069e-01 
  3 KSP Component U,V,W,P residual norm [ 1.614279704240e-04, 4.795734857592e-04, 1.613914106944e-04, 8.488885268501e-04 ]
        Residual norms for fieldsplit_u_ solve.
        0 KSP Residual norm 3.614067424860e+01 
        1 KSP Residual norm 1.727513541857e+00 
        2 KSP Residual norm 1.701193913599e-01 
  4 KSP Component U,V,W,P residual norm [ 1.182053444264e-04, 2.177408865879e-04, 1.181950335293e-04, 3.963394470286e-04 ]
	\end{verbatim}
\endgroup
The longer lines containing the ``{\tt U,V,W,P residual norm}'' relate to the residuals of the Stokes problem, whilst the shorter lines under each instance of ``{\tt Residual norms for fieldsplit\_u\_ solve.}'' relate to the viscous block solver which is performed as the Stokes preconditioner is applied.

\subsection{Trouble shooting convergence problems}

\subsubsection*{Convergence failure of Stokes solver}
\begin{enumerate}
	%%%%%%%%%%%%
	\item \emph{Elements possess a high aspect ratio (AR)}. When using $Q_2$-$P_1$ elements, the inf-sup constant increases with increasing AR. Possible fixes: (1) Strengthen the Schur complement preconditioner. This could be achieved by \unix{-fieldsplit\_p\_pc\_type lu}, or switching to use an approximate form  of $S = B^T A^{-1} B$ via say one application of a Krylov method preconditioned with the scaled lumped mass matrix. Increasing the strength of the Schur complement preconditioner can be achieved by making the solve on $A^{-1}$ more accurate and making the solve on $S$ more accurate; (2) modify the element resolution to make AR $< 10$.
	%%%%%%%%%%%%
	\item \emph{Convergence failure of the viscous block}. This can be diagnosed by running with these options; \unix{-fieldsplit\_u\_ksp\_monitor\_true\_residual -fieldsplit\_u\_ksp\_converged\_reason}. See below for more information.
	%%%%%%%%%%%%
\end{enumerate}


\subsubsection*{Convergence failure of viscous block}
\begin{enumerate}
	%%%%%%%%%%%%
	\item \emph{MG smoother is not strong enough}. Possible fixes: 
	(1) Increase the number of smoothing applications via \unix{-fieldsplit\_u\_mg\_levels\_ksp\_max\_it}; 
	(2) completely re-configure the smoother for example try \unix{-fieldsplit\_u\_mg\_levels\_ksp\_type fgmres -fieldsplit\_u\_mg\_levels\_ksp\_max\_it 4 -fieldsplit\_u\_mg\_levels\_pc\_type bjacobi}. Not that this will require assembling the coarse grid operators; 
	(3) Use more robust coarse grid operators, e.g. assemble some levels and use Galerkin coarsening on the coarser levels;
	(4) Use less levels, or try aggressive coarsening.
	%%%%%%%%%%%%
	\item \emph{MG coarse grid solver fails to converge}. 
	This can be diagnosed by running with these options; \
	\unix{-fieldsplit\_u\_mg\_coarse\_ksp\_monitor\_true\_residual -fieldsplit\_u\_mg\_coarse\_ksp\_converged\_reason}.
	Possible fixes: (1) Improve the coarse grid solver. See below for more details.
	%%%%%%%%%%%%
\end{enumerate}


\subsubsection*{Convergence failure of viscous block, GMG coarse grid solver}
\begin{enumerate}
	%%%%%%%%%%%%
	\item \emph{MG coarse grid preconditioner isn't strong enough}. Possible fixes: 
	(1) Use algebraic multigrid (ML, Hypre) as a preconditioner; 
	(2) Use heavy sub-domain preconditioners, e.g. \unix{-fieldsplit\_u\_mg\_coarse\_pc\_type asm -fieldsplit\_u\_mg\_coarse\_pc\_asm\_overlap 2 
	-fieldsplit\_u\_mg\_coarse\_sub\_pc\_type ilu -fieldsplit\_u\_mg\_coarse\_sub\_pc\_factor\_levels 2};
	(3) Try using a semi-redundant approach where you accumulate the coarse grid problem on a sub-set of cores and use stronger sub-domain preconditioners.
	%%%%%%%%%%%%
\end{enumerate}

\subsubsection*{Convergence failure of non-linear Stokes solver}
\begin{enumerate}
	%%%%%%%%%%%%
	\item \emph{Coming soon\dots}.
	%%%%%%%%%%%%
\end{enumerate}



\section{Geometry}

\subsection{Modelling domain}
The underlying coordinate system used in {\ptat} is Cartesian. In Fig.~\ref{fig:domain_coord} we identify the face labels used.
\begin{figure} [hbtp]
\center
\includegraphics[height=0.4\textheight]{figs/pt3d_coordinate_system.pdf}
\caption[\itshape ]
{\itshape Natural coordinate system used by the physical domain $\Omega$ in {\ptat}. Labels indicate various names given by different structures functionality.}
\label{fig:domain_coord}
\end{figure}

\begin{figure} [hbtp]
\center
\includegraphics[height=0.4\textheight]{figs/pt3d_basis_functions.pdf}
\caption[\itshape ]
{\itshape Nomenclature and basis function ordering for the volume and surface basis functions. $\xi, \eta, \zeta$ are the spatial coordinates in local element coordinate system. The notation $\xi^{+}, \xi^{-}$ indicates faces along which $\xi = 1, \xi = -1$ respectively. The convention associated with defining outward pointing surface normals $(\boldsymbol n)$ and the two tangent vectors to the surface $(\boldsymbol t_1,\boldsymbol t_2)$ are indicated.}
\label{fig:basis_functions}
\end{figure}

\subsection{Defining geometric regions on material points}

\section{Non-dimensional scaling}
The preferred choice of scaling is the following; the users chooses a characteristic velocity, length and viscosity $v^*$, $x^*$, $\eta^*$.
These choices define the following non-dimensional variables, $v' = v/v^*$, $x' = x/x^*$, $\eta' = \eta/\eta^*$. Given the Stokes problem
$$
	\nabla \cdot \eta \left( \nabla v + (\nabla v)^T \right) - \nabla p = \rho g,
	\qquad
	\nabla \cdot u = 0,
$$
the non-dimensional form thus becomes
$$
	\nabla' \cdot \eta' \left( \nabla' v' + (\nabla' v')^T \right) - \frac{x^*}{\eta^* v^*} \nabla' p = \frac{\phantom(x^*\phantom)^2}{\eta^* v^*} \rho g,
	\qquad
	\nabla' \cdot u' = 0,
$$
where $\nabla' = \frac{1}{x^*} \nabla$.
From this the non-dimensional pressure is chosen as 
$$
	p = p^* p', \qquad p^* = \frac{\eta^* v^*}{x^*} .
$$
Thus the final non-dimensional form of the Stokes problem is given by:
$$
	\nabla' \cdot \eta' \left( \nabla' v' + (\nabla' v')^T \right) - \nabla' p' 
	= \frac{\phantom(x^*\phantom)^2}{\eta^* v^*} \rho g
	\qquad
	\nabla' \cdot u' = 0.
$$

The right hand side should be interpreted as a non-dimensional force per volume. If desired, we can express the right hand side in terms of a non-dimensional density $\rho = \rho^* \rho'$ and gravity $g = g^* g'$.
$$
	\nabla' \cdot \eta' \left( \nabla' v' + (\nabla' v')^T \right) - \nabla' p' 
	= 	\left( \frac{\phantom(x^*\phantom)^2}{\eta^* v^*}  \right) 
		\left( \rho^* \rho' \right)  \left( g^* g' \right),
	\qquad
	\nabla' \cdot u' = 0.
$$

%% a = x.t^-2 = x (x/v)^-2 = x .v.v/(x.x)
\subsection{Additional notes}
\begin{enumerate}
% -------
	\item The ideal choice of $x^*, v^*, \eta^*$ scaling should result in values of velocity and pressure which follow $\bar{v} = \max_k(\vert v'_k \vert) \sim 1$ and $\bar{p} = \max_k(\vert p'_k \vert) \sim 1$. If your scaling doesn't produce this type of behaviour, then one should be careful in choosing the stopping condition applied to both the non-linear and linear solvers. For example, if $\bar{v} \sim 10^6$, it doesn't make much sense to solve the non-linear equations to a tolerance of $F_u, F_p \sim 10^{-9}$.
% -------
	\item Convergence of both linear and non-linear solver will appear slow if the scaling if the characteristic size of the velocity and pressure coming out of your model are orders of magnitude different. For instance, if your choice of scaling produces $F_u \sim 10^6$ and $F_p \sim 10^{-1}$ and you stop on $\delta_\text{abs} = 10^{-6}$, then a large number of iterations will be required to reduce the 2-norm of $(F_u, F_p)$ to be less than $10^{-6}$.
% -------
	\item Adjusting the scaling parameters to force $\bar{v}, \bar{p} \rightarrow 1$: Assuming a model with a linear rheology and a free surface boundary condition, the following observations can be made: (i) increasing $v^*$ by a factor of $X$ will cause $v'$ to \emph{decrease} by a factor of $X$ and $p'$ will be unaffected; (ii) increasing $x^*$ by a factor of $X$ will cause $v'$ to \emph{increase} by a factor of $X^2$ and $p'$ to \emph{increase} by a factor of $X^2$; (iii) increasing $\eta^*$ by a factor of $X$ will not affect $v'$, however $p'$ will \emph{decrease} by a factor of $X$.
\end{enumerate}

\end{document}
